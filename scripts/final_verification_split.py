#!/usr/bin/env python3
# scripts/final_verification.py
"""
FINAL VERIFICATION BEFORE TRAINING
- Quick check to confirm all datasets are ready
- No fixes needed - just validation
"""

import os
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
import warnings
warnings.filterwarnings('ignore')

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
SPLITS_DIR = os.path.join(BASE_DIR, "data", "splits")

def quick_verification():
    """Quick verification that everything is ready for training"""
    print("üöÄ FINAL TRAINING READINESS CHECK")
    print("=" * 60)
    
    all_good = True
    
    # Check directory structure
    print("\nüìÅ DIRECTORY STRUCTURE:")
    expected_datasets = ['heuristic', 'nlp', 'combined']
    for dataset in expected_datasets:
        dataset_dir = os.path.join(SPLITS_DIR, dataset)
        if os.path.exists(dataset_dir):
            npz_files = [f for f in os.listdir(dataset_dir) if f.endswith('.npz')]
            if len(npz_files) == 3:  # train, val, test
                print(f"‚úÖ {dataset:12} -> 3 NPZ files found")
            else:
                print(f"‚ùå {dataset:12} -> {len(npz_files)}/3 NPZ files")
                all_good = False
        else:
            print(f"‚ùå {dataset:12} -> MISSING")
            all_good = False
    
    # Quick data integrity check
    print("\nüîç DATA INTEGRITY CHECK:")
    for dataset in expected_datasets:
        if os.path.exists(os.path.join(SPLITS_DIR, dataset)):
            try:
                # Check one split per dataset (train split)
                train_path = os.path.join(SPLITS_DIR, dataset, f"{dataset}_train.npz")
                data = np.load(train_path, allow_pickle=True)
                
                # Reconstruct features
                features = csr_matrix(
                    (data['features_data'], data['features_indices'], data['features_indptr']),
                    shape=tuple(data['features_shape'])
                )
                
                # Basic checks
                n_samples = features.shape[0]
                n_features = features.shape[1]
                n_labels = len(data['labels'])
                
                if n_samples == n_labels:
                    print(f"‚úÖ {dataset:12} -> {n_samples:>7,} samples, {n_features:>4} features")
                else:
                    print(f"‚ùå {dataset:12} -> Sample mismatch: {n_samples} vs {n_labels}")
                    all_good = False
                    
            except Exception as e:
                print(f"‚ùå {dataset:12} -> Error: {e}")
                all_good = False
    
    # Check global summary
    print("\nüìä GLOBAL SUMMARY:")
    global_summary_path = os.path.join(SPLITS_DIR, "global_split_summary.csv")
    if os.path.exists(global_summary_path):
        df = pd.read_csv(global_summary_path)
        total_samples = df['samples'].sum()
        print(f"‚úÖ Global summary: {total_samples:,} total samples across all splits")
    else:
        print("‚ùå Global summary missing")
        all_good = False
    
    # Final recommendation
    print("\n" + "=" * 60)
    if all_good:
        print("üéâ PERFECT! READY FOR MODEL TRAINING! üöÄ")
        print("\nüìä YOUR DATASETS:")
        print("   ‚Ä¢ heuristic: 45 features")
        print("   ‚Ä¢ nlp:       1,806 features") 
        print("   ‚Ä¢ combined:  1,851 features")
        print(f"   ‚Ä¢ Total: ~1.6M samples (80/10/10 split)")
        print("\nüéØ NEXT STEP: Proceed with Random Forest training!")
    else:
        print("‚ùå Issues found - fix before training")
    
    return all_good

def show_training_recommendations():
    """Show recommendations for training"""
    print("\n" + "=" * 60)
    print("ü§ñ TRAINING RECOMMENDATIONS:")
    print("=" * 60)
    
    print("\nüîß RANDOM FOREST CONFIGURATION:")
    print("   ‚Ä¢ n_estimators: 100-200 (start with 100)")
    print("   ‚Ä¢ max_depth: 20-30 (prevents overfitting)")
    print("   ‚Ä¢ min_samples_split: 10-20")
    print("   ‚Ä¢ min_samples_leaf: 5-10")
    print("   ‚Ä¢ class_weight: 'balanced' (for your 85/15 distribution)")
    print("   ‚Ä¢ n_jobs: -1 (use all CPU cores)")
    print("   ‚Ä¢ random_state: 42 (for reproducibility)")
    
    print("\nüìà TRAINING STRATEGY:")
    print("   1. Start with heuristic features (fastest training)")
    print("   2. Then try NLP features (higher dimensionality)")
    print("   3. Finally combined features (best performance expected)")
    print("   4. Compare all three on validation set")
    print("   5. Select best model for final testing")
    
    print("\n‚ö° PERFORMANCE EXPECTATIONS:")
    print("   ‚Ä¢ Heuristic: Fast training, decent accuracy")
    print("   ‚Ä¢ NLP: Slower training, better accuracy") 
    print("   ‚Ä¢ Combined: Slowest training, best accuracy")
    print("   ‚Ä¢ Expected validation accuracy: 85-95%")
    
    print("\nüîç MODEL EVALUATION:")
    print("   ‚Ä¢ Use validation set for hyperparameter tuning")
    print("   ‚Ä¢ Use test set ONLY for final evaluation")
    print("   ‚Ä¢ Monitor: Accuracy, Precision, Recall, F1-score")
    print("   ‚Ä¢ Focus on malicious URL detection (recall)")

if __name__ == "__main__":
    ready = quick_verification()
    if ready:
        show_training_recommendations()